{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transliterate\n",
      "  Using cached transliterate-1.10.2-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: six>=1.1.0 in c:\\users\\vital\\anaconda3\\lib\\site-packages (from transliterate) (1.14.0)\n",
      "Installing collected packages: transliterate\n",
      "Successfully installed transliterate-1.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Using cached pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "Processing c:\\users\\vital\\appdata\\local\\pip\\cache\\wheels\\9b\\04\\dd\\7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\\docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "  Using cached pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "Collecting dawg-python>=0.7\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: docopt, pymorphy2-dicts, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transliterate\n",
    "from transliterate import translit, get_available_language_codes\n",
    "import pymorphy2\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тося"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачивание файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = []\n",
    "for i in os.walk('childes_Tosia'):\n",
    "    folder.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим список всех файлов и папок "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for address, dirs, files in folder:\n",
    "    for file in files:\n",
    "        filenames.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим список всех файлов, содержащих диалоги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []  # список готовых директорий к файлам с диалогами\n",
    "for filename in filenames:\n",
    "    if filename.startswith('TD'):\n",
    "        directory = 'childes_Tosia/malenkaya/'\n",
    "    else:\n",
    "        directory = 'childes_Tosia/'\n",
    "    path = directory + filename\n",
    "    paths.append(path)\n",
    "    #print(path)\n",
    "\n",
    "#print(len(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение реплик на то, что ребенок слышит, и то, что он говорит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "dict_words = []\n",
    "\n",
    "textChild = ''\n",
    "textParents = ''\n",
    "\n",
    "for filename in paths:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "        for line in file:\n",
    "            line = translit(line, 'ru')\n",
    "            line = re.sub('йо', 'ё', line)\n",
    "            if line.startswith('*ЦХИ'):  # выбираем только реплики ребенка \n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                textChild += line.lower()\n",
    "            elif line.startswith('*'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                textParents += line.lower()\n",
    "        \n",
    "with open('TosiaSays.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(textChild)\n",
    "with open('TosiaHears.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(textParents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsCounting(filepath):\n",
    "    with open(filepath, 'r+', encoding='utf-8') as f:\n",
    "        text = f.readlines()\n",
    "        # список из всех слов, которые говорит или слышит ребенок\n",
    "        words = []\n",
    "        for line in text:\n",
    "            if ' [: ' in line:\n",
    "                line = line.replace(' [:', 'ERR')\n",
    "            for word in line.lower().translate(str.maketrans(dict.fromkeys(string.punctuation))).split():\n",
    "                if not word.endswith('err'):\n",
    "                    words.append(word)\n",
    "\n",
    "    # список из словрей вида слово:часть речи\n",
    "    dict_words = [] \n",
    "    for word in words:\n",
    "        pos = morph.parse(word)[0].tag.POS\n",
    "        d = {}\n",
    "        d[word] = pos\n",
    "        dict_words.append(d)\n",
    "    \n",
    "    # список из частей речи всех слов -- для частеречного анализа\n",
    "    pos = []\n",
    "    len_corpus = 0\n",
    "    for d in dict_words:\n",
    "        for values in d.values():\n",
    "            len_corpus += 1\n",
    "            if values != None:\n",
    "                pos.append(values)\n",
    "    \n",
    "    # топ частей речи, которые слышит или говорит ребенок\n",
    "    top_pos = Counter(pos)\n",
    "    print('Распределение по частям речи ', top_pos)\n",
    "    \n",
    "    # общая размерность корпуса вместе со словами, у которых тэг части речи None\n",
    "    print('Всего слов в этой части коруса (вместе с неопределенной частью речи)', len_corpus)\n",
    "    \n",
    "    # топ-10 слов и топ-10 существительных, которые слышит или говорит ребенок\n",
    "    all_words = []\n",
    "    all_nouns = []\n",
    "    forbidden = ['цхи', 'ххх', 'мот', 'фат', 'æто',\n",
    "                 'лаугхинга', 'чико', 'нана', 'мотхер', 'бро',\n",
    "                 'мота', 'xxx', 'сис']\n",
    "    for d in dict_words:\n",
    "        for key, value in d.items():\n",
    "            key = morph.parse(key)[0].normal_form\n",
    "            if key not in forbidden:\n",
    "                all_words.append(key)\n",
    "            if value == 'NOUN' and key not in forbidden:\n",
    "                all_nouns.append(key)\n",
    "                \n",
    "    sw = stopwords.words('russian')\n",
    "    \n",
    "    filtered_words = [w for w in all_words if w not in sw]\n",
    "    filtered_nouns = [w for w in all_nouns if w not in sw]\n",
    "    \n",
    "    top_words = Counter(filtered_words)\n",
    "    top_words = dict(top_words)\n",
    "    top_words = {c: top_words[c] for c in top_words if top_words[c] > 2 and len(c) > 2}\n",
    "    top_words = (sorted(top_words.items(), key=lambda x: x[1], reverse=True))[:10]\n",
    "    \n",
    "    top_nouns = Counter(filtered_nouns)\n",
    "    top_nouns = dict(top_nouns)\n",
    "    top_nouns = {c: top_nouns[c] for c in top_nouns if top_nouns[c] > 2 and len(c) > 2}\n",
    "    top_nouns = (sorted(top_nouns.items(), key=lambda x: x[1], reverse=True))[:10]\n",
    "    \n",
    "    print('Топ-10 всех слов', top_words)\n",
    "    print('Топ-10 всех существительных', top_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что Тося говорит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение по частям речи  Counter({'NOUN': 2530, 'NPRO': 1327, 'VERB': 1259, 'PRCL': 1234, 'CONJ': 559, 'ADVB': 523, 'ADJF': 475, 'PRED': 464, 'INTJ': 343, 'PREP': 294, 'INFN': 209, 'NUMR': 75, 'ADJS': 57, 'GRND': 40, 'COMP': 27, 'PRTS': 7})\n",
      "Всего слов в этой части коруса (вместе с неопределенной частью речи) 21788\n",
      "Топ-10 всех слов [('xочить', 102), ('няня', 100), ('ешчё', 88), ('ама', 83), ('смотреть', 77), ('видеть', 76), ('мочь', 62), ('большой', 62), ('варя', 54), ('тося', 52)]\n",
      "Топ-10 всех существительных [('няня', 100), ('ешчё', 88), ('ама', 83), ('варя', 54), ('тося', 52), ('мама', 36), ('стасик', 33), ('метр', 25), ('папа', 24), ('цхилд', 21)]\n"
     ]
    }
   ],
   "source": [
    "WordsCounting('TosiaSays.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что Тося слышит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение по частям речи  Counter({'NOUN': 16218, 'VERB': 4498, 'CONJ': 3016, 'NPRO': 2884, 'PRCL': 2788, 'ADVB': 1942, 'PREP': 1725, 'ADJF': 1632, 'INFN': 902, 'INTJ': 662, 'PRED': 431, 'ADJS': 402, 'GRND': 223, 'NUMR': 168, 'COMP': 93, 'PRTS': 39, 'PRTF': 16})\n",
      "Всего слов в этой части коруса (вместе с неопределенной частью речи) 48988\n",
      "Топ-10 всех слов [('нанна', 567), ('тося', 454), ('давать', 301), ('смотреть', 163), ('ешчё', 161), ('мочь', 139), ('сказать', 135), ('весь', 135), ('тосй', 109), ('пойти', 108)]\n",
      "Топ-10 всех существительных [('нанна', 567), ('тося', 454), ('ешчё', 161), ('тосй', 109), ('мама', 102), ('тоня', 63), ('варя', 62), ('свет', 59), ('молодец', 54), ('ромашка', 48)]\n"
     ]
    }
   ],
   "source": [
    "WordsCounting('TosiaHears.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Яша"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачивание файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = []\n",
    "for i in os.walk('childes_Yasha'):\n",
    "    folder.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим список всех файлов и папок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for address, dirs, files in folder:\n",
    "    for file in files:\n",
    "        filenames.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим список всех файлов, содержащих диалоги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.txt', '11.txt', '12.txt', '13.06.2018.txt', '13.txt', '13_02_2018.txt', '14.txt', '15.txt', '16.txt', '18.txt', '19.txt', '20.txt', '21.cha', '22.cha', '22_02_2018.cha', '23.cha', '24.cha', '24_03_2018.cha', '25.cha', '26.cha', '27.01.2018.cha', '27.cha', '6_02_2018.txt', '7.txt', '8.txt']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "new = []\n",
    "for filename in filenames:\n",
    "    if filename[0].isnumeric():\n",
    "        new.append(filename)\n",
    "print(new)\n",
    "print(len(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []  # список готовых директорий к файлам с диалогами\n",
    "for filename in new:\n",
    "    directory = 'childes_Yasha/'\n",
    "    path = directory + filename\n",
    "    paths.append(path)\n",
    "    #print(path)\n",
    "\n",
    "#print(len(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение реплик на то, что ребенок слышит, и то, что он говорит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "dict_words = []\n",
    "\n",
    "for filename in paths:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "        textChild = ''\n",
    "        textParents = ''\n",
    "        for line in file:\n",
    "            line = translit(line, 'ru')\n",
    "            line = re.sub('йо', 'ё', line)\n",
    "            if line.startswith('*ЦХИ'):  # выбираем только реплики ребенка \n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                textChild += line.lower()\n",
    "            elif line.startswith('*'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                textParents += line.lower()\n",
    "        \n",
    "with open('YashaSays.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(textChild)\n",
    "with open('YashaHears.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(textParents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что Яша говорит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение по частям речи  Counter({'NOUN': 183, 'PREP': 53, 'INTJ': 33, 'VERB': 24, 'ADVB': 16, 'PRCL': 7, 'ADJF': 6, 'PRED': 5, 'NUMR': 4, 'CONJ': 3, 'INFN': 3, 'ADJS': 2})\n",
      "Всего слов в этой части коруса (вместе с неопределенной частью речи) 1421\n",
      "Топ-10 всех слов [('ала', 18), ('картошка', 15), ('деу', 14), ('мультик', 12), ('вилка', 10), ('дать', 10), ('пац', 7), ('алло', 7), ('спать', 6), ('мау', 6)]\n",
      "Топ-10 всех существительных [('ала', 18), ('картошка', 15), ('мультик', 12), ('вилка', 10), ('яша', 5), ('папа', 4), ('миша', 4), ('пакетик', 4), ('мусор', 3), ('клякляу', 3)]\n"
     ]
    }
   ],
   "source": [
    "WordsCounting('YashaSays.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что Яша слышит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение по частям речи  Counter({'NOUN': 1274, 'CONJ': 472, 'VERB': 452, 'NPRO': 300, 'PRCL': 277, 'ADVB': 155, 'PREP': 127, 'ADJF': 81, 'INFN': 63, 'INTJ': 60, 'PRED': 38, 'ADJS': 17, 'COMP': 8, 'NUMR': 6, 'GRND': 4, 'PRTS': 2, 'PRTF': 1})\n",
      "Всего слов в этой части коруса (вместе с неопределенной частью речи) 4813\n",
      "Топ-10 всех слов [('яша', 109), ('написать', 37), ('говорить', 30), ('давать', 28), ('сказать', 24), ('спать', 24), ('ешчё', 20), ('пожалуйста', 19), ('картошка', 18), ('мусор', 17)]\n",
      "Топ-10 всех существительных [('яша', 109), ('ешчё', 20), ('картошка', 18), ('мусор', 17), ('стол', 16), ('пупок', 16), ('салат', 12), ('буква', 12), ('коляска', 11), ('крокодил', 11)]\n"
     ]
    }
   ],
   "source": [
    "WordsCounting('YashaHears.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
