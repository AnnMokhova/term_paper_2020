{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transliterate import translit, get_available_language_codes\n",
    "import pymorphy2\n",
    "import os\n",
    "import re\n",
    "import transliterate\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Т. \n",
    "## Скачивание существительных для снятия частеречной омонимии "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = []\n",
    "for i in os.walk('childes_Tosia'):\n",
    "    folder.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for address, dirs, files in folder:\n",
    "    for file in files:\n",
    "        filenames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []  # список готовых директорий к файлам с диалогами\n",
    "for filename in filenames:\n",
    "    if filename.startswith('TD'):\n",
    "        directory = 'childes_Tosia/malenkaya/'\n",
    "    else:\n",
    "        directory = 'childes_Tosia/'\n",
    "    path = directory + filename\n",
    "    paths.append(path)\n",
    "    #print(path)\n",
    "\n",
    "#print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_says = {}\n",
    "words_hears = {}\n",
    "\n",
    "nouns_says = ''\n",
    "nouns_hears = ''\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for filename in paths:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "        for line in file:\n",
    "            line = translit(line, 'ru')\n",
    "            line = re.sub('йо', 'ё', line)\n",
    "            if line.startswith('*ЦХИ'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                for word in line.lower().translate(str.maketrans(dict.fromkeys(string.punctuation))).split():\n",
    "                    words_says[word] = filename\n",
    "            elif line.startswith('*'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                for word in line.lower().translate(str.maketrans(dict.fromkeys(string.punctuation))).split():\n",
    "                    #words_hears.append(word)\n",
    "                    words_hears[word] = filename\n",
    "\n",
    "for key, value in words_says.items():\n",
    "    pos = morph.parse(key)\n",
    "    if re.search('NOUN', str(pos)):\n",
    "        nouns_says += key + '\\t' + value + '\\t' + str(pos) + '\\n'\n",
    "\n",
    "for key, value in words_hears.items():\n",
    "    pos = morph.parse(key)\n",
    "    if re.search('NOUN', str(pos)):\n",
    "        nouns_hears += key + '\\t' + value + '\\t' + str(pos) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_as_nouns_Tosia_says.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(nouns_says)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_as_nouns_Tosia_hears.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(nouns_hears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Я.\n",
    "## Скачивание существительных для снятия частеречной омонимии "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = []\n",
    "for i in os.walk('childes_Yasha'):\n",
    "    folder.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for address, dirs, files in folder:\n",
    "    for file in files:\n",
    "        filenames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.txt', '11.txt', '12.txt', '13.06.2018.txt', '13.txt', '13_02_2018.txt', '14.txt', '15.txt', '16.txt', '18.txt', '19.txt', '20.txt', '21.cha', '22.cha', '22_02_2018.cha', '23.cha', '24.cha', '24_03_2018.cha', '25.cha', '26.cha', '27.01.2018.cha', '27.cha', '6_02_2018.txt', '7.txt', '8.txt']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "new = []\n",
    "for filename in filenames:\n",
    "    if filename[0].isnumeric():\n",
    "        new.append(filename)\n",
    "print(new)\n",
    "print(len(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []  # список готовых директорий к файлам с диалогами\n",
    "for filename in new:\n",
    "    directory = 'childes_Yasha/'\n",
    "    path = directory + filename\n",
    "    paths.append(path)\n",
    "    #print(path)\n",
    "\n",
    "#print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_says = {}\n",
    "words_hears = {}\n",
    "\n",
    "nouns_says = ''\n",
    "nouns_hears = ''\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for filename in paths:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        file = f.readlines()\n",
    "        for line in file:\n",
    "            line = translit(line, 'ru')\n",
    "            line = re.sub('йо', 'ё', line)\n",
    "            if line.startswith('*ЦХИ'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                for word in line.lower().translate(str.maketrans(dict.fromkeys(string.punctuation))).split():\n",
    "                    words_says[word] = filename\n",
    "            elif line.startswith('*'):\n",
    "                line = re.sub('\"*\".*', '', line)\n",
    "                line = re.sub('[0-9_]', '', line)\n",
    "                for word in line.lower().translate(str.maketrans(dict.fromkeys(string.punctuation))).split():\n",
    "                    #words_hears.append(word)\n",
    "                    words_hears[word] = filename\n",
    "\n",
    "for key, value in words_says.items():\n",
    "    pos = morph.parse(key)\n",
    "    if re.search('NOUN', str(pos)):\n",
    "        nouns_says += key + '\\t' + value + '\\t' + str(pos) + '\\n'\n",
    "\n",
    "for key, value in words_hears.items():\n",
    "    pos = morph.parse(key)\n",
    "    if re.search('NOUN', str(pos)):\n",
    "        nouns_hears += key + '\\t' + value + '\\t' + str(pos) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_as_nouns_Yasha_says.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(nouns_says)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagged_as_nouns_Yasha_hears.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(nouns_hears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
